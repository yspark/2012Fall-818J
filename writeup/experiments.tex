\section{Experimental Results}
\label{sec:experiments}

\subsection{Experimantal Setup}
In this milestone report, we show experimental results where we apply KNN method to sequences of CPU usage measurements data.
First, we select 10 movies from \term{Popular on Netflix} section provided by Netflix website and the list of selected movices are shown in Table \ref{tab:movies}.
Then we collect CPU usage statistics of each movie using Netflix application on Samsung Galaxy S device which is running Android 2.3 Gingerbread.
For the first 30 minutes of each movie, we measure CPU usage of Netflix at interval of 5 seconds and repeate the measurement 5 times.
Measurement results of two out of ten movies are previously shown in \ref{fig:preliminaries}.

\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|l|}
\hline
ID & Movie Title \\
\hline
1 & Transformers: Dark of the Moon \\
2 & Thor \\
3 & Hachi: A Dog's Tale \\
4 & The True Story of Puss 'n Boots \\
5 & Wallace \& Gromit: Loaf and Death \\
6 & Super 8 \\
7 & Mean Girls 2 \\
8 & Captain America \\
9 & Snatch \\
10 & No Strings Attached \\
\hline
\end{tabular}
\end{center}
\caption{Selected Movies}
\label{tab:movies}
\end{table}

For each movie, we select one of five CPU usage sequence as test data and take the rest four sequences as training data set. 
We set length of subsequence varying from 60 seconds to 360 seconds and then divide each test data and training data based on the selected length. 
When dividing sequence data, we adopt a concept of sliding window of size 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}
\subsection{Overview}
In this section, we describe 1) how CPU statistics for movies are collected, 2) how the collected raw data are processed to be useful in building up datasets and 3) how the processed data are stored in the datasets.

For the project milestone, we collected CPU statistics for 10 movies. Each statistic is captured by recording CPU statistics using the native UNIX command, Top, while playing 30-minute long video sequence. Video sequences and a query sequence are measured in 1 second interval which is the most fine-grained time interval in the Top command. To efficiently match a subsequence, a query sequence, to the collected video sequence in the dataset, we converted the time series datasets (i.e. the video sequence and the query sequence) into datasets in spatial space. To build-up the datasets largely consists of two jobs: pre-processing raw data sequences and storing the processed sequences into a data structure.
\begin{figure}[!ht]
\centering
\includegraphics[scale=0.35]{Figures/RAW_TF_Thor}
\caption{XXX}
\end{figure}

\begin{figure}[!ht]

	\subfigure[RAW] {
		\includegraphics[scale=0.5]{Figures/RAW_TF_Thor.eps}
		\label{fig:preliminary_thor}
	}
	\subfigure[SMTH_TF_Thor] {
		\includegraphics[scale=0.5]{Figures/SMTH_TF_Thor.eps}
		\label{fig:preliminary_thor}
	}
	\caption{CPU Usage Measurements of two movies}
	\label{fig:preliminaries}
\end{figure}

\subsection{Pre-processing raw data sequences}
For the same movie title, we collected cpu statistics for five times under the same environment. To remove the noise in the raw data, we used the Savitzky-Golay smoothing filter which is commonly used to filter out noise in time series data \cite{SGfilter}. The SG filter averages n adjacent points to flatten noise while preserving features of the distribution. In the experiment, we set the window length as 300. After smoothing, five cpu statistic streams were averaged because each statistic has little different amplitudes. Before averaging, streams needed to be aligned and trimmed appropriately because the data collection was manually done by hands. We set the first stream of each movie as a reference to the other 4 streams, and performed cross-correlation to obtain a time-lag which gives a point where streams are most similar. Once the alignment was done, the amplitude of five streams are averaged, and the sequence with the averaged amplitude was stored in the datasets and was used as the reference sequence to a query sequence.

To convert the pre-processed sequences into data trails in the spatial domain, Discrete Fourier Transform (DFT) was applied to the pre-processed sequences. More specifically, a  DFT window of a specified length is used so that DFT could be applied to each subsequence within a video sequence from the starting index to the end index of the video sequence. At every DFT application to each subsequence, the first two Fourier coefficients were extracted, which made the time series data sequence to be 2-dimensional spatial data while preserving more than 80-percent of energy of the original sequence.

\subsection{Storing processed sequence into a data structure} 
For an efficient search operation, we decided to use R* tree\cite{Beckmann:1990}, which is a variant of R-tree. For first-approximation of spatial query, R* tree stores Minimum Bounding Rectangles (MBRs) that consist of multiple adjacent points in multi-dimensional space. In our project, the pre-processed video sequence can be depicted as a trail, which is a series of two-dimensional spatial points corresponding to the Fourier coefficients of each subsequence within the video sequence. On packing MBRs, the packing strategy used in \cite{Faloutsos:1994}\cite{Kamel:1993} is adopted to minimize the disk access cost in accessing a specific node in R* tree.

\end{comment}