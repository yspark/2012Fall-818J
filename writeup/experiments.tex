\section{Experimental Setup}
\label{sec:experiments}

\subsection{Overview}
In this section, we describe 1) how CPU statistics for movies are collected, 2) how the collected raw data are processed to be useful in building up datasets and 3) how the processed data are stored in the datasets.

For the project milestone, we collected CPU statistics for 10 movies. Each statistic is captured by recording CPU statistics using the native UNIX command, Top, while playing 30-minute long video sequence. Video sequences and a query sequence are measured in 1 second interval which is the most fine-grained time interval in the Top command. To efficiently match a subsequence, a query sequence, to the collected video sequence in the dataset, we converted the time series datasets (i.e. the video sequence and the query sequence) into datasets in spatial space. To build-up the datasets largely consists of two jobs: pre-processing raw data sequences and storing the processed sequences into a data structure.

\subsection{Pre-processing raw data sequences}
For the same movie title, we collected cpu statistics for five times under the same environment. To remove the noise in the raw data, we used the Savitzky-Golay smoothing filter which is commonly used to filter out noise in time series data [REF: SG-Filter]. The SG filter averages n adjacent points to flatten noise while preserving features of the distribution. In the experiment, we set the window length as 300. After smoothing, five cpu statistic streams were averaged because each statistic has little different amplitudes. Before averaging, streams needed to be aligned and trimmed appropriately because the data collection was manually done by hands. We set the first stream of each movie as a reference to the other 4 streams, and performed cross-correlation to obtain a time-lag which gives a point where streams are most similar. Once the alignment was done, the amplitude of five streams are averaged, and the sequence with the averaged amplitude was stored in the datasets and was used as the reference sequence to a query sequence.

To convert the pre-processed sequences into data trails in the spatial domain, Discrete Fourier Transform (DFT) was applied to the pre-processed sequences. More specifically, a  DFT window of a specified length is used so that DFT could be applied to each subsequence within a video sequence from the starting index to the end index of the video sequence. At every DFT application to each subsequence, the first two Fourier coefficients were extracted, which made the time series data sequence to be 2-dimensional spatial data while preserving more than 80-percent of energy of the original sequence.

\subsection{Storing processed sequence into a data structure} 
For an efficient search operation, we decided to use R* tree [REF: R* tree, LIB webpage], which is a variant of R-tree. For first-approximation of spatial query, R* tree stores Minimum Bounding Rectangles (MBRs) that consist of multiple adjacent points in multi-dimensional space. In our project, the pre-processed video sequence can be depicted as a trail, which is a series of two-dimensional spatial points corresponding to the Fourier coefficients of each subsequence within the video sequence. On packing MBRs, the packing strategy used in [REF: On packing Rtree] is adopted to minimize the disk access cost in accessing a specific node in R* tree.