\subsection{K-Nearest Neighbor Algorithm}
\label{sec:knn} 

In order for CPU usage pattern matching, we adopted \term{k-nearest neighbor (KNN)} method, one of machine learning techniques widely used for pattern recognition.
KNN is a supervised machine learning and therefore it requires \term{training data set} which is pre-collected CPU usage statistics of target movies, and \term{test data set} on which KNN method should make a prediction based on the training data set.
If we give a unique class to each target movie, then our CPU usage pattern matching problem turns into classification problem. 
Once training data set is built up,  KNN method classifies a given test data based on the training data set. 

In our work, training data set and test data set consists of sequences of CPU usage measurements. 
However, we need to pre-process both training data set and test data because they usually contain different number of CPU measurement.
Practically, test data is collected from target users (or victims), and therefore we cannot expect that the length of test data is as long as that of our pre-built training data. 
Since KNN method assumes that both training data and test data are of the same length, somehow we need to modify both training and test data so that they have the same length. 
Therefore, we first need to define a length of subsequence and then divide test data and training data into several subsequences of the fixed length. 
We need to handle cases where a user watches a movie only for a while or watches several movies in a row. 
Therefore the subsequence length should be short enough since we cannot always expect that a user watches a movie for a long time using their mobile handset.  
On the other hand, if the subsequence length is too short, KNN method may not classify the test data due to lack of the number of features. 
Therefore, the subsequence length should be long enough so that test data can be successfully classified. 
Experimental results according to the length of subsequence is shown later in \ref{sec:experiments}.

In KNN method, a sequence of CPU usage measurements is considered as a vector and each CPU measurement is considered as a vector element. 
Then each subsequence of training and test data can be considered as a vector in a high dimensional space and the \term{Euclidean distance} between a given test data and pre-built training data set can be calculated using the below equation: 

\[
d( \vec{tr}, \vec{te} ) = [ \sum_{i=1}^{D} (\vec{tr}_i - \vec{te}_i)^2  ]^{(1/2)}
\]
where $\vec{tr}$ is a subsequence of training data, $\vec{te}$ is a subsequence of test data and $D$ corresponds to the length of subsequence. 
Then we can identify $K$ nearest neighbors of the test data and then make a prediction based on the majority class of the identified neighbors. 






\begin{comment}
There exist a few other machine learning methods such as \term{Support Vector Machine} or \term{Perceptron}, they have couple of drawbacks when applied to our pattern matching problem: 

(1) Features are not fixed 

Consider a sequence of CPU usage values as a vector and then KNN tries to find out k nearest vectors 
\end{comment}